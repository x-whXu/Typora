#### Xiao, Z., Weng, W., Zhang, Y., & Xiong, Z. (2022). EVA $^{2} $: Event-Assisted Video Frame Interpolation via Cross-Modal Alignment and Aggregation. *IEEE Transactions on Computational Imaging*, *8*, 1145-1158.



------

#### 1. Motivation

##### 1.1 Background

traditional video frame interpolation（VFI） methods很容易受到corruptions（影响）（比如：明显的非均匀运动、目标遮挡、环境光照变化、色彩变化），引入Event data能很好地缓解corruptions



##### 1.2 Event-assisted VFI的关键挑战：

a）cross-modal alignment：基于boundary keyframes和Event data，生成intermediate frame

previous methods没有从<u>image-level和feature-level</u>以及<u>不同scale</u>来align

<img src="2022 EVA2.assets/image-20230907202428502.png" alt="image-20230907202428502" style="zoom:67%;" />

b）cross-modal aggregate：aggregate aligned intermediate feature和event data

previous methods计算Dynamic convolution kernel时，没有考虑Event data，这算是没有充分考虑Event data和frames的内在联系

<img src="2022 EVA2.assets/image-20230907202445503.png" alt="image-20230907202445503" style="zoom:67%;" />



------

#### 2. Methods

在任意timestamp $\tau \in \left [ 0,1 \right ] $生成intermediate frame $\hat{I} _{\tau } $



------

##### 2.1 Input data

- 两个boundary keyframes $I_{0}, I_{1}\in  \mathbb{R} ^{3\times H\times W} $
- $\tau$左边和右边Event stream $E_{0\to \tau },E_{\tau \to 1} $，转为voxel grids $V_{0\to \tau },V_{\tau \to 1} \in  \mathbb{R} ^{B\times H\times W}$

<img src="2022 EVA2.assets/image-20230908210009059.png" alt="image-20230908210009059" style="zoom: 50%;" />



------

##### 2.2 Feature Encoder

##### <font color=Crimson>分别提取出image和Event模态的feature pyramid </font>

- 考虑到Event data和RGB data的domain gap，两者分别采用**两个**相同结构的Encoder
- 对于$I_{0}, I_{1}  (V_{0\to \tau },V_{\tau \to 1})$，Encoder参数共享

<img src="2022 EVA2.assets/image-20230908210749245.png" alt="image-20230908210749245" style="zoom:67%;" />



---

##### 2.3 Event-Guided Alignment (EGA) module

##### <font color=Crimson>对于每个scale，同时在image和feature level，估计每个position的offset，由此进行align（与利用optical flow进行align一样）</font>



a）RGB Image $I_{0}^{\downarrow s} $，RGB feature $F_{I_{0}}^{s} $，Event feature $F_{V_{0}\rightarrow \tau }^{s} $被concat起来Fusion

b）生成image-level和feature level的offset map $\bigtriangleup _{f,0\to \tau }^{s} $, $\bigtriangleup _{f,0\to \tau }^{s} \in \mathbb{R} ^{2\times \frac{H}{s} \times \frac{W}{s}}  $，offset map就是对每个pixel预测它align到intermediate frame时（h,w）的offset

c）Align function：**$F$**在（h,w）的值更新为：按offset map（h,w）位置的偏移值（$\bigtriangleup _{1hw},\bigtriangleup _{2hw} $）进行偏移后新坐标双线性插值的值

<img src="2022 EVA2.assets/image-20230909132946168.png" alt="image-20230909132946168" style="zoom: 33%;" />

d）通过Conv将image channel调整到Feaute channel

e）通过Concat和Conv操作，将aligned的image- and feature-level进行fusion

<img src="2022 EVA2.assets/image-20230909131506468.png" alt="image-20230909131506468" style="zoom:67%;" />



---

##### 2.4 Cross-Modal Event-Aware Dynamic Aggregation

##### <font color=Crimson>通过Dynamic Conv为每个位置的threshold建模，再次提升Align的效果（Dynamic Conv和Offset(optical flow)都是align操作，级联使用是为了互补提升align效果）</font>

a）EGA输出aligned特征$F_{Ali,0\to \tau }^{s} $和Event feature $F_{V_{0}\rightarrow \tau }^{s} $通过concat和Conv进行Fusion

b）生成Dynamic Convolution Kernel $\mathcal{F} $，对$F_{Ali,0\to \tau }^{s} $进行Dynamic Convolution

- 对于每个position，取出$\mathcal{F} $对应位置的channel feature，reshape成$k*k$卷积核，对$F_{Ali,0\to \tau }^{s} $对应位置区域进行卷积

c）通过Concat和Conv操作，将aggreated的image- and feature-level feature进行fusion

<img src="2022 EVA2.assets/image-20230909133345263.png" alt="image-20230909133345263" style="zoom:67%;" />



---

##### 2.6 Loss function

Charbonnier loss：生成的intermediate frame和groundtruth计算L2 loss

<img src="2022 EVA2.assets/image-20230909134531165.png" alt="image-20230909134531165" style="zoom:67%;" />



---

##### 2.7 Ablation study

|      | EVA2 w/o EDA | EVA2 w/o EGA | EGA w/o Image | EGA w/o Feature | EGA w/o Event | EDA w/o attention | EDA w/o concat |
| ---- | ------------ | ------------ | ------------- | --------------- | ------------- | ----------------- | -------------- |
|      | 0.14         | 0.23         | 0.21          | 0.1             | 0.53          | 0.23              | 0.14           |

