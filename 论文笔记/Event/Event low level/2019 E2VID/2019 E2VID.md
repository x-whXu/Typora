#### Rebecq, H., Ranftl, R., Koltun, V., & Scaramuzza, D. (<font color=Crimson>2019</font>). <font color=Crimson>Events-to-video: Bringing modern computer vision to event cameras.</font> In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition* (pp. 3857-3866).



------

#### 1. Motivation

##### 1.1 他们工作是什么？为什么？

尽管Event by event方法和Event frames、Time surfaces等Group of event方法已经取得impressive的进展，但是他们并非nature image，无法直接利用conventional computer vision方法处理下游任务。因此，**他们选择Event to video Task**，从a stream of events重建natural videos，在事件相机视觉和传统计算机视觉之间建立一座桥梁。

<img src="2019 E2VID.assets/image-20230830192722632.png" alt="image-20230830192722632" style="zoom:50%;" />

##### 1.2 为什么需要建模Event to video Task，而不是直接积分Event to video?

threshold C在image plane既不恒定也不均匀，受到polarity、event rate（pixel bandwidth的限制）、temperature等各种因素的影响。因此，不能直接通过integrate event来重建intensity images，而需要estimate。

（猜测：利用simulated event data训练，主要是在建模simulation中的threshold分布（N(0.18, 0.03)）？因此可以引入realistic reconstruction task?）



**1.3 与previous methods的区别：**

a)  previous methods大多是基于small window of events，重建single image；而他们是重建相同时长的video。

b)  previous methods主要是利用handcrafted priors；而他们是从大量的simulated event data中数据驱动式地直接学习。

**c)  他们主要关注下游任务效果，而不是重建质量。**



------

#### 2. Methods

##### 2.1 作者认为为什么需要synthetic event data?

a)  需要**大量**成对的 (voxel, image) 数据，获取困难

b)  **在Event camera表现出色的场景（HDR，高速），standard camera只能获取到poor ground truth**



##### 2.2 Synthetic event data的制作细节

- 采用event simulator ESIM
- MS-COCO Dataset
- 将image映射到3D plane中，模拟随机摄像机运动触发的事件
- 每个模拟场景使用单独的**一组**正负threshold，从N(0.18, 0.03)（经验参数）中采样，以避免model直接积分event

（怎么从单张COCO图片simulate出video 序列的？）



##### 2.3 Event Representation : voxel grid --> tensor

a)  将stream of event按每N (25000) 个一组进行划分，每组确定一张image（由Event rate确定frames rate）

b)  按该组最大时间差ΔT均匀量化为B (10) 份

- 把t~i~从 [t~N-1~, t~0~] normalize到 [0, B-1] 范围得到t~i~^*^
- (x~l~, y~m~) 位置 t~n~时刻的 voxel为 (x~l~, y~m~) 位置上t~i~^*^介于[t~n-1~, t~n+1~]之间Event的有极性和



##### 2.4 Model architecture

a)  Unet：4个strided convolution layers（down） + two residual blocks（mid） + 4个upsampling transposed convolution layers（up） + Depth Convolution（out）

b)  输入：当前组的Event序列和先前K张重建图片的concat	(B + K) × H × W

c)  recurrent connection：每次使用新的事件序列增量更新新的重建图片

<img src="2019 E2VID.assets/image-20230831135520716.png" alt="image-20230831135520716" style="zoom: 50%;" />



##### 2.5 Loss function

**每个**voxel tensor（每组Event）和Ground Truth的image计算Perceptual Similarity (LPIPS)

